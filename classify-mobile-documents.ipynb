{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['index','text', 'tag']\n",
    "data = pd.DataFrame( columns=columns)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row(text, classes, df):\n",
    "    df.loc[len(df)] = [len(df), utils.to_utf8(text, errors='replace').decode(\"utf8\"), classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "dirpath = 'data/train/shortstory/'\n",
    "shortstory = [add_row(textract.process(dirpath + f),'shortstory', data) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "dirpath = 'data/train/java/'\n",
    "java = [add_row(textract.process(dirpath + f), 'technical', data) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "dirpath = 'data/train/python/'\n",
    "python = [add_row(textract.process(dirpath + f),'technical', data) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "dirpath = 'data/train/medicine/'\n",
    "medicine = [add_row(textract.process(dirpath + f),'medicine', data) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "dirpath = 'data/train/mobilescreen/'\n",
    "mobilescreen = [add_row(textract.process(dirpath + f),'mobilescreen', data) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "dirpath = 'data/train/mobilememorycard/'\n",
    "mobilememorycard = [add_row(textract.process(dirpath + f),'mobilememorycard', data) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "dirpath = 'data/train/lcd/'\n",
    "mobilememorycard = [add_row(textract.process(dirpath + f),'lcdscreen', data) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "dirpath = 'data/train/hippa/'\n",
    "mobilememorycard = [add_row(textract.process(dirpath + f),'hippa', data) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Tags to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['tags1'] = [ [x] for x in data.tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(data.tags1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stemming.porter2 import stem\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "def review_to_wordlist( text, remove_stopwords=True ):\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if not w in cachedStopWords]\n",
    "    words = (list(map(lambda token: stem(lemma.lemmatize(token)),words)))\n",
    "    \n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import string\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "def review_to_sentences( text, remove_stopwords=True ):\n",
    "    text = filter(lambda x: x in printable, text.strip())\n",
    "    raw_sentences = tokenizer.tokenize(text)\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.extend( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Simple timing decorator\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start  = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta  = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timeit\n",
    "def build_and_evaluate(X, y, classifier=SGDClassifier, outpath=None, verbose=True):\n",
    "\n",
    "    @timeit\n",
    "    def build(classifier, X, y=None):\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = OneVsSGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.9,\n",
    "                    eta0=0.2, fit_intercept=True, l1_ratio=0,\n",
    "                    learning_rate='optimal', loss='modified_huber', n_iter=10, n_jobs=1,\n",
    "                    penalty='elasticnet', power_t=0.6, random_state=None, shuffle=True,\n",
    "                    verbose=0, warm_start=False)\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', None),\n",
    "            ('vectorizer', TfidfVectorizer(tokenizer=review_to_sentences, stop_words=stop_words, lowercase=False)),\n",
    "            ('classifier', classifier),\n",
    "        ])\n",
    "\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "#    labels = LabelEncoder()\n",
    "    labels = MultiLabelBinarizer()\n",
    "    y = labels.fit_transform(y)\n",
    "\n",
    "    if verbose: print(\"Building for evaluation\")\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)\n",
    "    model, secs = build(classifier, X_train, y_train)\n",
    "\n",
    "    if verbose: print(\"Evaluation model fit in {:0.3f} seconds\".format(secs))\n",
    "    if verbose: print(\"Classification Report:\\n\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "    print('y_test: ', y_test)\n",
    "    print('value :' , labels.inverse_transform(y_test))\n",
    "    print('y_pred: ', y_pred)\n",
    "    print('value :' , labels.inverse_transform(y_pred))\n",
    "    if verbose: print(\"Building complete model and saving ...\")\n",
    "    model, secs = build(classifier, X, y)\n",
    "    model.labels_ = labels\n",
    "    \n",
    "    if verbose: print(\"Complete model fit in {:0.3f} seconds\".format(secs))\n",
    "\n",
    "    if outpath:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        print(\"Model written out to {}\".format(outpath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_most_informative_features(model, text=None, n=20):\n",
    "\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "    classifier = model.named_steps['classifier']\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not hasattr(classifier, 'coef_'):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {} model.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        tvec = classifier.coef_\n",
    "\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    coefs = sorted(\n",
    "        zip(tvec[0], vectorizer.get_feature_names()),\n",
    "        key=itemgetter(0), reverse=True\n",
    "    )\n",
    "\n",
    "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        output.append(\"Classified as: {}\".format(model.predict([text])))\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(cp, fnp, cn, fnn)\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['index','text', 'tag']\n",
    "test = pd.DataFrame( columns=columns)\n",
    "dirpath = 'data/test/medicine/'\n",
    "[add_row(textract.process(dirpath + f),'medicine', test) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "dirpath = 'data/test/mobilescreen/'\n",
    "[add_row(textract.process(dirpath + f),'mobilescreen', test) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "dirpath = 'data/test/mobilememorycard/'\n",
    "[add_row(textract.process(dirpath + f),'mobilememorycard', test) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "dirpath = 'data/test/lcd/'\n",
    "[add_row(textract.process(dirpath + f),'lcdscreen', test) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "dirpath = 'data/test/java/'\n",
    "[add_row(textract.process(dirpath + f),'technical', test) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "dirpath = 'data/test/hippa/'\n",
    "[add_row(textract.process(dirpath + f),'hippa', test) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    PATH = \"model.pickle\"\n",
    "\n",
    "    if not os.path.exists(PATH):\n",
    "        # Time to build the model\n",
    "        X = data.text\n",
    "        y = data.tag\n",
    "\n",
    "        model = build_and_evaluate(X,y, outpath=PATH)\n",
    "\n",
    "    else:\n",
    "        with open(PATH, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            vectorizer = model.named_steps['vectorizer']\n",
    "            classifier = model.named_steps['classifier']\n",
    "            Xte = vectorizer.transform(test.text)\n",
    "            y_pred = classifier.predict(Xte)\n",
    "            print(y_pred)\n",
    "            labels = model.labels_\n",
    "            print(labels.inverse_transform(y_pred))\n",
    "            print(test.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stop_words = stopwords.words(\"english\")    \n",
    "vectorizer = TfidfVectorizer(tokenizer=review_to_sentences, stop_words=stop_words, lowercase=False,use_idf=True)\n",
    "X = vectorizer.fit_transform(data.text.str.upper())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(data.tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = LabelBinarizer()  #MultiLabelBinarizer() #LabelEncoder() #\n",
    "y = labels.fit_transform(data.tag)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = OneVsRestClassifier(SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.9,\n",
    "                    eta0=0.2, fit_intercept=True, l1_ratio=0,\n",
    "                    learning_rate='optimal', loss='modified_huber', n_iter=10, n_jobs=1,\n",
    "                    penalty='elasticnet', power_t=0.6, random_state=None, shuffle=True,\n",
    "                    verbose=0, warm_start=False))\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xte = vectorizer.transform(test.text)\n",
    "y_pred = classifier.predict(Xte)\n",
    "y_pred_prob = classifier.predict_proba(Xte)\n",
    "print(y_pred)\n",
    "print(y_pred_prob)\n",
    "print(labels.inverse_transform(y_pred))\n",
    "print(test.tag.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = pd.DataFrame({'yte': test.tag.tolist(), 'ypred' : labels.inverse_transform(y_pred)})\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
