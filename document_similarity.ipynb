{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read all documents which are added to text.tsv\n",
    "This text.tcv is created by textvectoring file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sandoz plans weedkiller joint venture in ussr....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>taiwan rejects textile makers exchange rate pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>national fsi inc nfsi&gt; 4th qtr loss. shr loss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>occidental oxy&gt; official resigns. midcon corp,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>italy's bnl to issue 120 mln dlr convertible b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence                                               text\n",
       "0         1  sandoz plans weedkiller joint venture in ussr....\n",
       "1         2  taiwan rejects textile makers exchange rate pl...\n",
       "2         3  national fsi inc nfsi> 4th qtr loss. shr loss ...\n",
       "3         4  occidental oxy> official resigns. midcon corp,...\n",
       "4         5  italy's bnl to issue 120 mln dlr convertible b..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pd.read_csv('data/text.tsv', delimiter='\\t', header=None, names=[\"sequence\", \"text\"])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read respective tags of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>usa,ussr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>usa,taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>earn,usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>italy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence        tags\n",
       "0         1    usa,ussr\n",
       "1         2  usa,taiwan\n",
       "2         3    earn,usa\n",
       "3         4         usa\n",
       "4         5       italy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pd.read_csv('data/oldfiles/tags.tsv', delimiter='\\t', header=None, names=[\"sequence\", \"tags\"])\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### create lemmtized text to root word"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "text['text_lem'] = [''.join([WordNetLemmatizer().\n",
    "                             lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists])\n",
    "                                                .strip() for lists in text['text']]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add tags to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text['tags'] = tags['tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### binary encode the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "Y = lb.fit_transform(text.tags)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create TFidfVectore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19716, 5000)\n"
     ]
    }
   ],
   "source": [
    "# below parameters gives correct cosine prediction\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    max_features=5000,\n",
    "    min_df=10,\n",
    "    stop_words='english',\n",
    "    use_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(text['text'].str.lower())\n",
    "print(X.shape)\n",
    "tfidf = X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    " \n",
    "def tokenize(text):\n",
    "    min_length = 3\n",
    "    words = map(larmbda word: word.lower(), word_tokenize(text));\n",
    "    words = [word for word in words\n",
    "                  if word not in cachedStopWords]\n",
    "    tokens =(list(map(lambda token: PorterStemmer().stem(token),\n",
    "                  words)));\n",
    "    p = re.compile('[a-zA-Z]+');\n",
    "    filtered_tokens = list(filter(lambda token:\n",
    "                  p.match(token) and len(token)>=min_length,\n",
    "         tokens));\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# get the TF-IDF of the text\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, min_df=3,\n",
    "                        max_df=0.90, max_features=3000,\n",
    "                        use_idf=True, sublinear_tf=True,\n",
    "                        norm='l2')\n",
    "X = vectorizer.fit_transform(text.text)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calcuate Cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19716, 19716)\n",
      "(19716, 19716)\n"
     ]
    }
   ],
   "source": [
    "#pairwise_similarity = ((tfidf * tfidf.T).A)\n",
    "pairwise_similarity = cosine_similarity(X)\n",
    "print(pairwise_similarity.shape)\n",
    "dist = 1 - pairwise_similarity\n",
    "print(dist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying Dimention Reduction - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pos = pca.fit_transform(dist)\n",
    "xs, ys = pos[:,0], pos[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get values which have cosine greater than 0.32 for first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For max_feature = 5000\n",
    "for j in range(20):\n",
    "    for i in range(len(pairwise_similarity)):\n",
    "        if( pairwise_similarity[j][i] > 0.32):\n",
    "            print(i , \" \", pairwise_similarity[j][i], \" \" , tags['tags'][i])\n",
    "    print('*******************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(pairwise_similarity)):\n",
    "        if( pairwise_similarity[116][i] > 0.32):\n",
    "            print(i , \" \", pairwise_similarity[116][i], \" \" , tags['tags'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cosine silimarity more than 0.32 is predicting correct documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = int(.8 * X.shape[0])\n",
    "Xtr, ytr = X[:N], Y[:N]\n",
    "Xte, yte = X[N:], Y[N:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " just ignore warnings of ill-defined recall/precision etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# logistic regression parameter to optimise\n",
    "params = {\"estimator__C\": np.logspace(1, 1.5, num=5)}\n",
    "# use OneVsRestClassifier for multiclass learning\n",
    "model = OneVsRestClassifier(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "import numpy as np\n",
    "\n",
    "# use OneVsRestClassifier for multiclass learning\n",
    "model = OneVsRestClassifier(GradientBoostingClassifier(random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "import numpy as np\n",
    "\n",
    "# use OneVsRestClassifier for multiclass learning\n",
    "model = OneVsRestClassifier(SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit = model.fit(Xtr,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xte.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ypred = fit.predict(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3944"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = []\n",
    "for n in xrange(len(yte)):\n",
    "    tags.append((lb.classes_[yte[n]==1], lb.classes_[ypred[n]==1]))\n",
    "    \n",
    "ans = pd.DataFrame(tags, columns=['actualtags', 'predictedtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualtags</th>\n",
       "      <th>predictedtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[earn,italy]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[uk]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nickel,uk]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[hong-kong]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[earn,netherlands]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[money-fx,dlr,usa,west-germany,poehl]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[acq,usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[earn,usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[japan]</td>\n",
       "      <td>[interest,usa, usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[botswana]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[uk]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[iran,iraq,bahrain]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[west-germany,oecd]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[acq,uk,usa]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[thailand,brunei,malaysia,indonesia,singapore,...</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[uk,japan,tse]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[acq,usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[china,france]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[groundnut,japan,usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[australia]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[tanzania]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[earn,usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[west-germany]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>[grain,wheat,barley,corn,france,ec]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>[usa,sweden]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>[acq,usa]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[earn,usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>[grain,wheat,usa]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>[acq,usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>[acq,usa,canada]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>[livestock,hog,carcass,usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[earn,usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>[interest,uk]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>[usa,japan,nakasone]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[earn,usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>[crude,egypt,opec]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>[grain,barley,uk,ireland]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[earn,usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[earn,usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>[acq,usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>[earn,usa]</td>\n",
       "      <td>[earn,usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>[sugar,uk,jordan]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>[acq,usa]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>[poland]</td>\n",
       "      <td>[usa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>[switzerland]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>[luxembourg]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>[sugar,iraq,iran,uk,turkey]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            actualtags        predictedtags\n",
       "0                                         [earn,italy]                   []\n",
       "1                                                 [uk]                   []\n",
       "2                                          [nickel,uk]                   []\n",
       "3                                          [hong-kong]                [usa]\n",
       "4                                   [earn,netherlands]                   []\n",
       "5                [money-fx,dlr,usa,west-germany,poehl]                [usa]\n",
       "6                                            [acq,usa]                   []\n",
       "7                                           [earn,usa]                   []\n",
       "8                                           [earn,usa]           [earn,usa]\n",
       "9                                              [japan]  [interest,usa, usa]\n",
       "10                                          [botswana]                   []\n",
       "11                                                [uk]                   []\n",
       "12                                               [usa]                   []\n",
       "13                                 [iran,iraq,bahrain]                   []\n",
       "14                                 [west-germany,oecd]                [usa]\n",
       "15                                        [acq,uk,usa]                [usa]\n",
       "16   [thailand,brunei,malaysia,indonesia,singapore,...                [usa]\n",
       "17                                               [acq]                   []\n",
       "18                                      [uk,japan,tse]                [usa]\n",
       "19                                               [usa]                   []\n",
       "20                                               [usa]                   []\n",
       "21                                               [usa]                   []\n",
       "22                                           [acq,usa]                   []\n",
       "23                                      [china,france]                [usa]\n",
       "24                               [groundnut,japan,usa]                   []\n",
       "25                                         [australia]                [usa]\n",
       "26                                          [tanzania]                [usa]\n",
       "27                                          [earn,usa]           [earn,usa]\n",
       "28                                               [usa]                   []\n",
       "29                                      [west-germany]                [usa]\n",
       "..                                                 ...                  ...\n",
       "270                [grain,wheat,barley,corn,france,ec]                   []\n",
       "271                                       [usa,sweden]                   []\n",
       "272                                          [acq,usa]                [usa]\n",
       "273                                              [acq]                   []\n",
       "274                                         [earn,usa]           [earn,usa]\n",
       "275                                              [usa]                [usa]\n",
       "276                                  [grain,wheat,usa]                [usa]\n",
       "277                                          [acq,usa]                   []\n",
       "278                                   [acq,usa,canada]                   []\n",
       "279                        [livestock,hog,carcass,usa]                   []\n",
       "280                                         [earn,usa]           [earn,usa]\n",
       "281                                      [interest,uk]                [usa]\n",
       "282                               [usa,japan,nakasone]                [usa]\n",
       "283                                         [earn,usa]           [earn,usa]\n",
       "284                                 [crude,egypt,opec]                   []\n",
       "285                          [grain,barley,uk,ireland]                   []\n",
       "286                                              [usa]                   []\n",
       "287                                         [earn,usa]           [earn,usa]\n",
       "288                                              [usa]                   []\n",
       "289                                         [earn,usa]           [earn,usa]\n",
       "290                                          [acq,usa]                   []\n",
       "291                                         [earn,usa]           [earn,usa]\n",
       "292                                  [sugar,uk,jordan]                   []\n",
       "293                                          [acq,usa]                [usa]\n",
       "294                                              [usa]                   []\n",
       "295                                           [poland]                [usa]\n",
       "296                                      [switzerland]                   []\n",
       "297                                              [acq]                   []\n",
       "298                                       [luxembourg]                   []\n",
       "299                        [sugar,iraq,iran,uk,turkey]                   []\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.head(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### do a grid search on the params, with 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14934077079107505"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(yte, ypred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf = GridSearchCV(model, param_grid=params, scoring='f1', n_jobs=-1, cv=5)\n",
    "clf.fit(Xtr, ytr)\n",
    "clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute F1 Score usinf TP,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# compute predictions on test set\n",
    "pred = clf.predict(Xte)\n",
    "# compute F1-score on test set\n",
    "f1_score(yte, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare with tags in test data\n",
    "tags = []\n",
    "for n in xrange(20):\n",
    "    tags.append((lb.classes_[yte[n]==1], lb.classes_[pred[n]==1]))\n",
    "    \n",
    "pd.DataFrame(tags, columns=['actual tags', 'predicted tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to get cosine similarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#nltk.download('punkt') # if necessary..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "\n",
    "print cosine_sim('a little bird', 'a little bird')\n",
    "print cosine_sim('a little bird', 'a little bird chirps')\n",
    "print cosine_sim('a little bird', 'a big dog barks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
